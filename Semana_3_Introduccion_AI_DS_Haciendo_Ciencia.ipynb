{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semana 3: Implementación de Modelos de Machine learning\n",
        "\n",
        "\n",
        "### Objetivos:\n",
        "*   Explorar el uso de árboles de Decisión (supervisado)\n",
        "*   Explorar el uso de random forest (supervisado)\n",
        "*   Explorar el uso de máquinas de soporte vectorial (supervisado)\n",
        "*   Explorar el uso de análisis de componentes principales (supervisado)\n",
        "*   Explorar el uso de clustering por k-means (no supervisado)\n"
      ],
      "metadata": {
        "id": "X9XkjC7nccq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Análisis Exploratorio de Datos**:\n",
        "\n"
      ],
      "metadata": {
        "id": "T2FXmTR6fN8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')  # 0: maligno, 1: benigno"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fCsfUH-XZKXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame completo con características y objetivo\n",
        "df = pd.concat([X, y], axis=1)"
      ],
      "metadata": {
        "id": "77cJCArYTEaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lqbIBoXmTFPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Información general del dataset\n",
        "print(\"=== Información general del dataset ===\")\n",
        "print(f\"Número de muestras: {df.shape[0]}\")\n",
        "print(f\"Número de características: {df.shape[1] - 1}\")\n",
        "print(f\"Clases objetivo: {data.target_names}\")\n",
        "print(\"\\nPrimeras 5 filas del dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "C2hds3CzTIVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Resumen estadístico\n",
        "print(\"\\n=== Resumen estadístico de las características ===\")\n",
        "print(X.describe())"
      ],
      "metadata": {
        "id": "rurl2RAaTKnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Distribución de la variable objetivo\n",
        "print(\"\\n=== Distribución de la variable objetivo ===\")\n",
        "print(y.value_counts(normalize=True))\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='target', data=df)\n",
        "plt.title('Distribución de Clases (0: Maligno, 1: Benigno)')\n",
        "plt.xlabel('Clase')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.xticks(ticks=[0, 1], labels=['Maligno', 'Benigno'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "49zcYyXpTMhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Verificación de valores faltantes\n",
        "print(\"\\n=== Valores faltantes ===\")\n",
        "print(X.isnull().sum().sum(), \"valores faltantes en total\")"
      ],
      "metadata": {
        "id": "vjfM521oTOoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Distribución de algunas características (seleccionamos las primeras 5 para visualización)\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, col in enumerate(X.columns[:5]):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.histplot(X[col], kde=True)\n",
        "    plt.title(f'Distribución de {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frecuencia')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zZaVYNTxTQjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Correlación entre características\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = X.corr()\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "plt.title('Matriz de Correlación de Características')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SmV3_LXLTTXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Correlaciones más fuertes\n",
        "print(\"\\n=== Correlaciones más fuertes (absolutas) ===\")\n",
        "corr_unstack = correlation_matrix.abs().unstack()\n",
        "corr_sorted = corr_unstack.sort_values(ascending=False)\n",
        "corr_sorted = corr_sorted[corr_sorted < 1.0]  # Excluir correlaciones de una variable consigo misma\n",
        "print(corr_sorted.head(10))"
      ],
      "metadata": {
        "id": "x3X13jZeTWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Boxplots de características clave por clase (seleccionamos las primeras 5 para visualización)\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, col in enumerate(X.columns[:5]):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.boxplot(x='target', y=col, data=df)\n",
        "    plt.title(f'{col} por Clase')\n",
        "    plt.xlabel('Clase (0: Maligno, 1: Benigno)')\n",
        "    plt.xticks(ticks=[0, 1], labels=['Maligno', 'Benigno'])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1o4QyYlLTZ7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modelos:\n",
        "\n",
        "- Árboles de Decisión\n",
        "- Random Forest\n",
        "- Máquinas de Vectores de Soporte\n",
        "- Análisis de Componentes Principales\n",
        "- Clustering K-Means\n"
      ],
      "metadata": {
        "id": "8nsMAMP3genf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.1. Árboles de Decisión**\n",
        "\n",
        "\n",
        "#### **¿Cómo Funcionan los Árboles de Decisión?**\n",
        "\n",
        "Los Árboles de Decisión operan haciendo una serie de preguntas sobre las características de los datos. Cada pregunta corresponde a una división en el árbol, guiando los datos por diferentes ramas hasta que se toma una decisión o predicción final en un nodo hoja.\n",
        "\n",
        "En un Árbol de Decisión, cada nodo interno representa una prueba en una característica específica (por ejemplo, \"¿El ancho del pétalo es > 1.0 cm?\"). Las ramas representan los posibles resultados de esa prueba (por ejemplo, \"Sí\" o \"No\"). Los nodos hoja al final de las ramas representan la predicción final. Para la clasificación, esta predicción es la etiqueta de clase (por ejemplo, \"Iris-setosa\" o \"Iris-versicolor\"). Para la regresión, la predicción es un valor continuo (por ejemplo, un precio de vivienda previsto). La estructura similar a un diagrama de flujo de un Árbol de Decisión lo hace altamente interpretable, lo que le permite comprender fácilmente el razonamiento detrás de sus predicciones.\n",
        "\n",
        "#### **Conceptos Matemáticos Subyacentes**\n",
        "\n",
        "Al construir un Árbol de Decisión, el algoritmo necesita decidir en qué característica dividir en cada nodo. El objetivo es realizar divisiones que resulten en los nodos más \"puros\", es decir, nodos que contengan principalmente puntos de datos pertenecientes a una sola clase (para clasificación) o que tengan una baja varianza en la variable objetivo (para regresión). Entropía, Ganancia de Información e Impureza de Gini son métricas utilizadas para medir esta pureza o impureza.\n",
        "\n",
        "**La entropía** es una medida del desorden o la aleatoriedad en un nodo. Un nodo con una mezcla de diferentes clases tiene una entropía alta, mientras que un nodo que contiene solo una clase tiene una entropía cero.\n",
        "\n",
        "**La ganancia de información** mide la reducción de la entropía lograda después de dividir un nodo en una característica particular. El algoritmo tiene como objetivo elegir la característica que produce la mayor ganancia de información, ya que esta división separa mejor los datos en función de la variable objetivo.\n",
        "\n",
        "**La impureza de Gini** es otra métrica que mide la probabilidad de clasificar erróneamente un elemento elegido al azar si se etiquetara de acuerdo con la distribución de clases en el nodo. Una impureza de Gini de 0 indica un nodo puro.\n",
        "\n",
        "El algoritmo del Árbol de Decisión **selecciona iterativamente la característica que maximiza la Ganancia de Información o minimiza la Impureza de Gini para dividir los datos en cada nodo**, con el objetivo de crear un árbol que clasifique o prediga eficazmente la variable objetivo. Estas métricas proporcionan una forma matemática de determinar las \"mejores\" preguntas para hacer en cada paso de la construcción del árbol, asegurando que las divisiones sean informativas y conduzcan a predicciones precisas.\n",
        "\n",
        "#### **Supuestos Clave de los Árboles de Decisión**\n",
        "\n",
        "Un supuesto fundamental de los algoritmos básicos de Árboles de Decisión (como ID3) es que se prefiere que los valores de las características sean categóricos. Si los valores son continuos, a menudo se discretizan en categorías antes de construir el modelo. Sin embargo, las implementaciones modernas como el algoritmo CART (utilizado en scikit-learn) pueden manejar características numéricas directamente encontrando puntos de división óptimos.\n",
        "\n",
        "Otro supuesto es que todo el conjunto de entrenamiento se considera inicialmente como el nodo raíz. Luego, el árbol crece dividiendo recursivamente esta raíz en subárboles en función de los valores de los atributos.\n",
        "\n",
        "Los Árboles de Decisión también suponen que los registros se distribuyen recursivamente en función de los valores de los atributos.\n",
        "\n",
        "El orden en que los atributos se colocan como raíz o nodos internos se determina utilizando enfoques estadísticos como la Ganancia de Información o la Impureza de Gini.\n",
        "\n",
        "#### **Para verificar la idoneidad de su conjunto de datos para los Árboles de Decisión**\n",
        "\n",
        "1. Para las características categóricas, utilice value_counts() en Pandas para ver la distribución de las categorías. Las distribuciones muy sesgadas podrían afectar el equilibrio del árbol.\n",
        "\n",
        "2. Para las características numéricas, utilice histogramas (por ejemplo, con matplotlib.pyplot.hist()) para visualizar su distribución. Los Árboles de Decisión pueden manejar varias distribuciones, pero siempre es útil comprender la dispersión de los datos.\n",
        "\n",
        "3. Considere las relaciones entre las características. Si bien los Árboles de Decisión pueden capturar relaciones no lineales, las características altamente correlacionadas podrían conducir a divisiones redundantes. Puede verificar la correlación utilizando una matriz de correlación (por ejemplo, df.corr() en Pandas) y un mapa de calor.\n",
        "\n",
        "4. Si bien la implementación del Árbol de Decisión de scikit-learn es bastante flexible, comprender estos supuestos subyacentes puede ayudar a interpretar el modelo y potencialmente mejorar su rendimiento mediante la ingeniería de características o eligiendo algoritmos alternativos si los supuestos se violan gravemente.\n",
        "\n",
        "\n",
        "\n",
        "#### **Resumen**\n",
        "\n",
        "Los árboles de decisión dividen los datos en subconjuntos basados en características, maximizando la pureza de cada nodo.\n",
        "\n",
        "#### a. **Impureza de Gini**\n",
        "\n",
        "Mide la probabilidad de clasificar incorrectamente una observación si se elige una clase al azar.\n",
        "\n",
        "$Gini(D) = 1 - ∑_{i=1}^{k} p_i^2$\n",
        "\n",
        "Donde:\n",
        "\n",
        "D: conjunto de datos en un nodo.\n",
        "\n",
        "$p_i$: proporción de muestras de la clase i en el nodo.\n",
        "\n",
        "#### b. **Ganancia de Información (Information Gain)**\n",
        "\n",
        "Elije la característica que mejor divide los datos:\n",
        "\n",
        "$Gain(D,A) = Impureza(D) - ∑_{v∈Valores(A)} \\frac{|D_v|}{|D|} \\cdot Impureza(D_v)$\n",
        "\n",
        "$A$: característica evaluada.\n",
        "\n",
        "$D_v$: subconjunto de datos con valor v en la característica A."
      ],
      "metadata": {
        "id": "kG-Lwhug9xer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Características más importantes según un árbol de decisión simple\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X, y)\n",
        "feature_importance = pd.Series(dt.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\n=== Importancia de las características (Árbol de Decisión) ===\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance[:10].plot(kind='bar')\n",
        "plt.title('Top 10 Características más Importantes (Árbol de Decisión)')\n",
        "plt.xlabel('Característica')\n",
        "plt.ylabel('Importancia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "diXmmP7lThVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cómo funciona dt.feature_importances_:\n",
        "\n",
        "1. Los árboles de decisión asignan un valor de importancia a cada característica basado en cuánto contribuye cada una a reducir la impureza (por ejemplo, el índice de Gini) en los nodos del árbol.\n",
        "\n",
        "2. La importancia se calcula sumando la reducción de impureza ponderada por la proporción de muestras que pasan por cada nodo donde se usa la característica.\n",
        "\n",
        "3. Los valores están normalizados para que sumen 1 (es decir, son proporciones relativas).\n",
        "\n",
        "Ejemplo: Si worst radius tiene una importancia de 0.7, significa que contribuye con el 7% de la capacidad del árbol para separar las clases.\n",
        "\n"
      ],
      "metadata": {
        "id": "hPERFMvRUhpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el conjunto de datos de cáncer de mama\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target  # Variable objetivo (0: maligno, 1: benigno)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Crear un objeto clasificador de Árbol de Decisión\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Entrenar el clasificador\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión del modelo: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Ej43dyxcSpHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2. Random Forest: Construyendo sobre Árboles de Decisión**\n",
        "\n",
        "\n",
        "#### **¿Cómo Funcionan los Random Forest**\n",
        "\n",
        "Un Random Forest es como tener una colección (o \"bosque\") de muchos Árboles de Decisión trabajando juntos para hacer una predicción. En lugar de depender de la predicción de un solo Árbol de Decisión, el Random Forest agrega las predicciones de múltiples árboles para llegar a un resultado más robusto y preciso.\n",
        "\n",
        "La idea central detrás de Random Forest es una técnica de aprendizaje de conjunto llamada **Bagging**, que significa Bootstrap Aggregating. Así es como funciona:\n",
        "\n",
        "1. **Muestreo Bootstrap**: El algoritmo Random Forest crea múltiples subconjuntos de los datos de entrenamiento originales mediante un muestreo aleatorio con reemplazo. Esto significa que algunos puntos de datos pueden incluirse varias veces en un subconjunto, mientras que otros pueden omitirse.\n",
        "\n",
        "2. **Selección Aleatoria de Características**: Al construir cada Árbol de Decisión en el bosque, el algoritmo selecciona aleatoriamente solo un subconjunto de las características disponibles para considerar la división en cada nodo. Esto introduce una mayor diversidad entre los árboles.\n",
        "\n",
        "3. **Construcción Independiente de Árboles**: Cada Árbol de Decisión en el bosque se construye de forma independiente en su propia muestra bootstrap y utilizando el subconjunto de características seleccionado aleatoriamente.\n",
        "\n",
        "4. **Agregación de Predicciones**: Al realizar una predicción para un nuevo punto de datos, cada árbol en el bosque realiza su propia predicción. Para las tareas de clasificación, el Random Forest toma un voto mayoritario (la clase predicha por la mayoría de los árboles es la predicción final). Para las tareas de regresión, promedia las predicciones de todos los árboles.\n",
        "\n",
        "Al **introducir aleatoriedad** tanto en el muestreo de datos como en la selección de características, los Random Forest **reducen** la tendencia de los Árboles de Decisión individuales a **sobreajustar los datos de entrenamiento**, lo que lleva a una mejor generalización y una mayor precisión en los datos no vistos.\n",
        "\n",
        "\n",
        "\n",
        "#### **Conceptos Matemáticos Subyacentes**\n",
        "\n",
        "El principio matemático detrás de la efectividad de combinar múltiples árboles en un Random Forest radica en la **reducción de la varianza**. Se sabe que los **Árboles de Decisión individuales** son modelos de \"alta varianza\", lo que significa que sus predicciones pueden **cambiar significativamente con pequeñas variaciones en los datos de entrenamiento**.\n",
        "\n",
        "\n",
        "Al **promediar las predicciones (para la regresión)** o **tomar un voto mayoritario o la moda (para la clasificación)** de muchos árboles entrenados de forma independiente, el **Random Forest suaviza efectivamente estas variaciones individuales**, lo que resulta en una predicción más estable y confiable. Si bien el sesgo (el error de las suposiciones demasiado simplistas en el algoritmo de aprendizaje) del Random Forest es similar al de un solo Árbol de Decisión, la reducción significativa de la varianza a menudo conduce a un rendimiento general mucho mejor.\n",
        "\n",
        "#### **Supuestos Clave de los Árboles de Decisión**\n",
        "\n",
        "Al igual que los Árboles de Decisión, los Random Forest funcionan bien con **datos que tienen valores reales** (No NaN ni estandarizados) en las variables de características. Si faltan en gran medida valores de características o no tienen sentido, es probable que los árboles construidos sobre estos datos no sean confiables.\n",
        "\n",
        "Un supuesto clave que distingue a los Random Forest es que **las predicciones de cada árbol individual deben tener correlaciones muy bajas**. La aleatoriedad introducida a través del muestreo bootstrap y la selección aleatoria de características durante la construcción del árbol ayuda a garantizar esta baja correlación. Si los árboles están altamente correlacionados (por ejemplo, si las características fuertes dominan consistentemente las divisiones en cada árbol), el beneficio del ensamble disminuye.\n",
        "\n",
        "Los Random Forest también suponen que **no hay una fuerte multicolinealidad** (alta correlación entre las características) en el conjunto de datos. Si bien la selección aleatoria de características ayuda a mitigar el impacto de algunas características correlacionadas, una **multicolinealidad muy alta aún puede afectar la eficiencia e interpretabilidad del modelo**.\n",
        "\n",
        "Al igual que los Árboles de Decisión, los Random Forest **pueden manejar eficazmente relaciones no lineales en los datos**  sin requerir transformaciones explícitas de las características.\n",
        "\n",
        "La **eficacia de un Random Forest depende en gran medida de la diversidad de los árboles individuales**. Asegurar una baja correlación entre sus predicciones es crucial para lograr ganancias de rendimiento significativas sobre un solo Árbol de Decisión.\n",
        "\n",
        "#### **Para verificar la idoneidad de su conjunto de datos para los Árboles de Decisión**\n",
        "\n",
        "1. Para las características categóricas, utilice value_counts() en Pandas para ver la distribución de las categorías. Las distribuciones muy sesgadas podrían afectar el equilibrio del árbol.\n",
        "\n",
        "2. Para las características numéricas, utilice histogramas (por ejemplo, con matplotlib.pyplot.hist()) para visualizar su distribución. Los Árboles de Decisión pueden manejar varias distribuciones, pero siempre es útil comprender la dispersión de los datos.\n",
        "\n",
        "3. Considere las relaciones entre las características. Si bien los Árboles de Decisión pueden capturar relaciones no lineales, las características altamente correlacionadas podrían conducir a divisiones redundantes. Puede verificar la correlación utilizando una matriz de correlación (por ejemplo, df.corr() en Pandas) y un mapa de calor.\n",
        "\n",
        "4. Si bien la implementación del Árbol de Decisión de scikit-learn es bastante flexible, comprender estos supuestos subyacentes puede ayudar a interpretar el modelo y potencialmente mejorar su rendimiento mediante la ingeniería de características o eligiendo algoritmos alternativos si los supuestos se violan gravemente.\n",
        "\n",
        "\n",
        "\n",
        "#### **Resumen**\n",
        "\n",
        "Combina múltiples árboles de decisión mediante bagging (bootstrap aggregation).\n",
        "\n",
        "#### a. **Media o Votación Mayoritaria**\n",
        "\n",
        "Mide la probabilidad de clasificar incorrectamente una observación si se elige una clase al azar.\n",
        "\n",
        "Para regresión:\n",
        "\n",
        "$\\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$\n",
        "\n",
        "Para clasificación:\n",
        "\n",
        "$\\hat{y} = \\text{moda}(y_1, y_2, \\dots, y_T)$\n",
        "\n",
        "Donde:\n",
        "\n",
        "$T$: número de árboles.\n",
        "\n",
        "$y_t$: predicción del árbol t."
      ],
      "metadata": {
        "id": "dYxCBnOVBI4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "NosAiaqISDbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "q4GeS9AbSMPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el conjunto de datos de cáncer de mama\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target  # Variable objetivo (0: maligno, 1: benigno)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Crear un objeto clasificador Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Entrenar el clasificador\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Precisión del modelo: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "v7Y-lNoVRJ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3. Máquinas de Vectores de Soporte (SVM): Encontrando el Límite Óptimo**\n",
        "\n",
        "\n",
        "#### **¿Cómo Funcionan las Máquinas de Vectores de Soporte?**\n",
        "\n",
        "La idea fundamental detrás de SVM para clasificación es **encontrar la mejor línea** (o en dimensiones superiores, un hiperplano) que **separe los puntos de datos pertenecientes a diferentes clases** con la **mayor brecha posible entre ellos**. Esta brecha se llama margen. Los **puntos de datos que se encuentran más cerca de este hiperplano óptimo se denominan vectores de soporte**. Estos vectores de soporte son cruciales ya que definen la posición y la orientación del hiperplano.\n",
        "\n",
        "**Para la regresión**, conocida como Regresión de Vectores de Soporte (SVR), SVM utiliza principios similares. **En lugar de encontrar un hiperplano que separe las clases**, SVR tiene como objetivo **encontrar una función** que **mejor se ajuste a los valores de salida continuos dentro de un cierto margen de error (a menudo denominado tubo épsilon)**. El objetivo es que tantos puntos de datos como sea posible se encuentren dentro de este margen alrededor de la función predicha.\n",
        "\n",
        "El enfoque en **maximizar el margen en la clasificación hace que las SVM sean robustas a datos nuevos y no vistos**, ya que crea una separación clara entre las clases. En la regresión, el concepto de tubo épsilon proporciona flexibilidad en el ajuste de los datos.\n",
        "\n",
        "\n",
        "#### **Conceptos Matemáticos Subyacentes**\n",
        "\n",
        "Matemáticamente, el objetivo de un clasificador SVM es **encontrar el hiperplano** que no solo **separa las clases**, sino que también **maximiza la distancia desde el hiperplano** hasta los puntos de datos más cercanos de cualquier clase **(los vectores de soporte)**. Esto se logra definiendo un vector de peso (w) que es perpendicular al hiperplano y un término de sesgo (b) que determina el desplazamiento del hiperplano desde el origen. Luego, el algoritmo intenta encontrar los valores de w y b que satisfagan las restricciones de clasificación (clasificar correctamente todos los datos de entrenamiento) al tiempo que maximizan la magnitud del margen, que es inversamente proporcional a la magnitud del vector de peso (w).\n",
        "\n",
        "\n",
        "\n",
        "#### **Supuestos Clave de los SVM**\n",
        "\n",
        "1. Las SVM suponen que los **puntos de datos son independientes e idénticamente distribuidos (i.i.d.)**. Esta es una suposición común en muchos algoritmos de aprendizaje automático.\n",
        "\n",
        "2. Una de las fortalezas de las **SVM es su efectividad en espacios de alta dimensión**, incluso cuando el número de dimensiones es mayor que el número de muestras.\n",
        "\n",
        "3. La SVM lineal funciona mejor cuando los datos son linealmente separables, lo que significa que las diferentes clases pueden separarse perfectamente mediante una línea recta (en 2D) o un hiperplano (en dimensiones superiores).\n",
        "\n",
        "Sin embargo, los datos del mundo real a menudo no son linealmente separables. Para manejar tales casos, las SVM utilizan una técnica poderosa llamada truco del kernel. Las funciones kernel mapean los datos originales a un espacio de mayor dimensión donde se vuelven linealmente separables. Las funciones kernel comunes incluyen:\n",
        "\n",
        "a. **Kernel Lineal**: Equivalente a una SVM lineal sin ninguna transformación.\n",
        "\n",
        "b. **Kernel Polinómico**: Introduce no linealidad al considerar combinaciones polinómicas de las características.\n",
        "\n",
        "c. **Kernel de Función de Base Radial (RBF)**: Un kernel muy popular que puede manejar límites no lineales complejos mapeando los datos a un espacio de dimensión infinita.\n",
        "\n",
        "d. **Kernel Sigmoide**: Similar a la función de activación de una red neuronal.\n",
        "\n",
        "El truco del kernel es una razón clave de la versatilidad y el poder de las SVM, lo que les permite abordar una amplia gama de problemas de clasificación y regresión, incluso aquellos con relaciones complejas y no lineales entre las características.\n",
        "\n",
        "#### **Resumen**\n",
        "\n",
        "Encuentra un hiperplano que maximiza el margen entre clases.\n",
        "\n",
        "#### a. **Función de Decisión Lineal**\n",
        "\n",
        "$f(x) = \\text{sign}(w \\cdot x + b)$\n",
        "\n",
        "Donde:\n",
        "\n",
        "$w$: vector de pesos (perpendicular al hiperplano).\n",
        "\n",
        "$b$: sesgo (intercepto).\n",
        "\n",
        "$x$: características de entrada.\n",
        "\n",
        "#### b. **Optimización de Margen**\n",
        "\n",
        "Minimiza: $\\frac{1}{2} \\|w\\|^2$\n",
        "\n",
        "Sujeto a: $y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i$\n",
        "\n",
        "Donde:  \n",
        "- $y_i \\in \\{-1, 1\\}$: etiquetas de clase.  \n",
        "- Los puntos en la frontera son **vectores de soporte**.\n",
        "\n",
        "#### c. **Kernel Trick (Ejemplo: RBF)**\n",
        "\n",
        "$K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
        "\n",
        "Donde:  \n",
        "- $\\gamma$: parámetro de escala del kernel."
      ],
      "metadata": {
        "id": "bye7gdEIGWPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train SVM classifiers (Linear and RBF kernels)\n",
        "svm_linear = svm.SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = svm.SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate models\n",
        "def print_metrics(y_true, y_pred, model_name):\n",
        "    print(f\"\\n{model_name} Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "print_metrics(y_test, y_pred_linear, \"Linear SVM\")\n",
        "print_metrics(y_test, y_pred_rbf, \"RBF SVM\")\n",
        "\n",
        "# Cross-validation scores\n",
        "cv_scores_linear = cross_val_score(svm_linear, X_train_scaled, y_train, cv=5)\n",
        "cv_scores_rbf = cross_val_score(svm_rbf, X_train_scaled, y_train, cv=5)\n",
        "print(f\"\\nLinear SVM CV Scores: {cv_scores_linear.mean():.4f} (±{cv_scores_linear.std() * 2:.4f})\")\n",
        "print(f\"RBF SVM CV Scores: {cv_scores_rbf.mean():.4f} (±{cv_scores_rbf.std() * 2:.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mQFL0i3lQUtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix for Linear SVM\n",
        "cm = confusion_matrix(y_test, y_pred_linear)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Linear SVM')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig('confusion_matrix_linear_svm.png')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YV2MmvOdQoF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot feature importance for Linear SVM\n",
        "coef = np.abs(svm_linear.coef_[0])\n",
        "feature_importance = sorted(zip(coef, feature_names), reverse=True)[:10]\n",
        "coef_values, feature_names_top = zip(*feature_importance)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names_top, coef_values)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Top 10 Feature Importance - Linear SVM')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_linear_svm.png')\n"
      ],
      "metadata": {
        "id": "4WKFuTi7QpxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos de muestra\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel()\n",
        "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Crear un objeto regresor SVR con un kernel RBF\n",
        "svr_regressor = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "\n",
        "# Entrenar el regresor\n",
        "svr_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = svr_regressor.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Error cuadrático medio: {mse}\")\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_test, y_test, color='darkorange', label='datos')\n",
        "plt.plot(X_test, y_pred, color='red', lw=2, label='modelo RBF')\n",
        "plt.xlabel('datos')\n",
        "plt.ylabel('objetivo')\n",
        "plt.title('Regresión de Vectores de Soporte')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWYFQb8ZQN9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.4. Análisis de Componentes Principales (PCA): Reducción de Dimensionalidad**\n",
        "\n",
        "\n",
        "#### **¿Cómo funciona PCA?**\n",
        "\n",
        "**PCA** logra esto transformando las características originales en un nuevo conjunto de características no correlacionadas llamadas componentes principales (CP). Estos componentes principales se ordenan según la cantidad de varianza que explican en los datos. El primer componente principal captura la mayor varianza, el segundo captura la siguiente mayor, y así sucesivamente. PCA ayuda a simplificar conjuntos de datos complejos identificando los patrones subyacentes más importantes, lo que puede ser útil para la visualización, la aceleración de los algoritmos de aprendizaje automático y la reducción del ruido\n",
        "\n",
        "\n",
        "#### **Conceptos Matemáticos Subyacentes**\n",
        "\n",
        "En esencia, PCA implica encontrar las direcciones en los datos que tienen la mayor dispersión. Estas direcciones están representadas por vectores propios. La importancia o la cantidad de varianza explicada por cada vector propio viene dada por su correspondiente valor propio. Una mayor valor propio significa que el vector propio correspondiente (componente principal) captura más varianza de los datos. La Relación de Varianza Explicada es la proporción de la varianza total en el conjunto de datos que es explicada por cada componente principal. Al examinar los valores propios y la relación de varianza explicada, puede determinar cuántos componentes principales se necesitan para conservar una cantidad significativa de información de los datos originales.\n",
        "\n",
        "\n",
        "#### **Supuestos Clave de los PCA**\n",
        "\n",
        "\n",
        "#### **Resumen**\n",
        "\n",
        "Reduce dimensionalidad proyectando datos en direcciones de máxima varianza.\n",
        "\n",
        "#### a. **Matriz de Covarianza**\n",
        "\n",
        "$\\Sigma = \\frac{1}{n-1} X^T X$\n",
        "\n",
        "Donde:\n",
        "\n",
        "- $X$: datos centrados (media 0).\n",
        "\n",
        "#### b. **Autovectores y Autovalores**\n",
        "\n",
        "Encuentra $\\lambda$ y $v$ tales que:  \n",
        "$\\Sigma v$ = $\\lambda v$\n",
        "\n",
        "Donde:  \n",
        "- Los autovectores v son las **componentes principales**.  \n",
        "- Los autovalores \\lambda representan la **varianza explicada**.\n",
        "\n",
        "#### c. **Proyección**\n",
        "\n",
        "$Y = X \\cdot V_k$\n",
        "\n",
        "Donde:  \n",
        "- $V_k$: matriz con los primeros k autovectores."
      ],
      "metadata": {
        "id": "EKPFgX6MMwT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Cargar el conjunto de datos de cáncer de mama\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aplicar PCA para reducir a 2 componentes\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Crear un DataFrame para los componentes principales\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "pca_df['target'] = y\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['target'], cmap='viridis')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.title('PCA del Conjunto de Datos de Cáncer de Mama')\n",
        "plt.legend(*scatter.legend_elements(), title='Clases', labels=['Maligno', 'Benigno'])\n",
        "plt.show()\n",
        "\n",
        "# Relación de varianza explicada\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(f\"Relación de Varianza Explicada: {explained_variance_ratio}\")\n",
        "print(f\"Porcentaje de varianza explicada por PC1: {explained_variance_ratio[0]*100:.2f}%\")\n",
        "print(f\"Porcentaje de varianza explicada por PC2: {explained_variance_ratio[1]*100:.2f}%\")\n",
        "print(f\"Varianza total explicada: {sum(explained_variance_ratio)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "aJxTJLxrQK1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Composición de los componentes principales\n",
        "# Obtener los vectores de carga (loadings) de las características\n",
        "loadings = pd.DataFrame(pca.components_.T, index=cancer.feature_names, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Mostrar los loadings\n",
        "print(\"\\n=== Composición de los Componentes Principales (Loadings) ===\")\n",
        "print(loadings)\n",
        "\n",
        "# Visualizar la contribución de las características a PC1 y PC2\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico de barras para PC1\n",
        "plt.subplot(1, 2, 1)\n",
        "loadings['PC1'].abs().sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title('Contribución de Características a PC1')\n",
        "plt.xlabel('Característica')\n",
        "plt.ylabel('Magnitud del Loading (absoluto)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Gráfico de barras para PC2\n",
        "plt.subplot(1, 2, 2)\n",
        "loadings['PC2'].abs().sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title('Contribución de Características a PC2')\n",
        "plt.xlabel('Característica')\n",
        "plt.ylabel('Magnitud del Loading (absoluto)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identificar las características más influyentes\n",
        "print(\"\\n=== Top 5 características con mayor contribución a PC1 ===\")\n",
        "print(loadings['PC1'].abs().sort_values(ascending=False).head(5))\n",
        "print(\"\\n=== Top 5 características con mayor contribución a PC2 ===\")\n",
        "print(loadings['PC2'].abs().sort_values(ascending=False).head(5))"
      ],
      "metadata": {
        "id": "lHVuJEzbU8Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering K-Means: Agrupando Puntos de Datos Similares\n",
        "\n",
        "\n",
        "### **2.1.5. Clustering K-Means: Agrupando Puntos de Datos Similares**\n",
        "\n",
        "\n",
        "#### **¿Cómo funciona K-Means?**\n",
        "\n",
        "**K-Means** funciona identificando 'K' centroides, que son puntos representativos para cada clúster. El algoritmo sigue un proceso iterativo:\n",
        "\n",
        "1. **Inicialización**: Se eligen inicialmente 'K' centroides al azar (o utilizando un método más sofisticado como K-Means++).\n",
        "Asignación: Cada punto de datos en el conjunto de datos se asigna al clúster cuyo centroide está más cerca, generalmente utilizando la distancia euclidiana.\n",
        "2. **Actualización**: Los centroides de los clústeres se recalculan como la media de todos los puntos de datos asignados a ese clúster.\n",
        "\n",
        "3. **Iteración**: Los pasos 2 y 3 se repiten hasta que los centroides ya no cambian significativamente o se alcanza un número predefinido de iteraciones.\n",
        "\n",
        "K-Means tiene como objetivo agrupar puntos de datos similares, revelando estructuras o segmentos subyacentes en los datos.\n",
        "\n",
        "#### **Conceptos Matemáticos Subyacentes**\n",
        "\n",
        "Matemáticamente, el objetivo del algoritmo **K-Means** es minimizar la **suma de errores cuadráticos dentro del clúster (inercia)**. La inercia **mide la suma de las distancias cuadradas entre cada punto de datos y el centroide de su clúster más cercano**. Un valor de inercia más bajo indica que los puntos de datos están más agrupados dentro de sus respectivos clústeres\n",
        "\n",
        "\n",
        "#### **Supuestos Clave de los PCA**\n",
        "\n",
        "**K-Means** asume que los clústeres son esféricos y tienen un tamaño similar. También supone que todos los clústeres tienen la misma varianza. El algoritmo es sensible a los valores atípicos. Además, K-Means funciona mejor con datos numéricos y puede requerir escalado para garantizar que las características con valores más grandes no dominen el proceso de clustering\n",
        "\n",
        "#### **Resumen**\n",
        "\n",
        "Reduce dimensionalidad proyectando datos en direcciones de máxima varianza.\n",
        "\n",
        "#### a. **Función de Costo (Inercia)**\n",
        "\n",
        "$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$\n",
        "\n",
        "Donde:  \n",
        "- $C_i$: cluster i.  \n",
        "- $\\mu_i$: centroide de C_i.\n",
        "\n",
        "#### b. **Distancia Euclidiana**\n",
        "\n",
        "$d(x, \\mu_i) = \\sqrt{\\sum_{j=1}^{d} (x_j - \\mu_{ij})^2}$\n",
        "\n",
        "Donde:  \n",
        "- $d$: número de características."
      ],
      "metadata": {
        "id": "80gam8FGOZ0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar datos de muestra\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Crear un DataFrame para facilitar el análisis\n",
        "df = pd.DataFrame(X, columns=['Feature_1', 'Feature_2'])\n",
        "df['True_Cluster'] = y  # Etiquetas verdaderas de los clústeres\n",
        "\n",
        "# 1. Información general del dataset\n",
        "print(\"=== Información general del dataset ===\")\n",
        "print(f\"Número de muestras: {df.shape[0]}\")\n",
        "print(f\"Número de características: {df.shape[1] - 1}\")\n",
        "print(f\"Clústeres verdaderos: {np.unique(y)}\")"
      ],
      "metadata": {
        "id": "0egPSwclVYGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Resumen estadístico\n",
        "print(\"\\n=== Resumen estadístico ===\")\n",
        "print(df[['Feature_1', 'Feature_2']].describe())"
      ],
      "metadata": {
        "id": "6q_8wbrEVbs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Distribución de los clústeres verdaderos\n",
        "print(\"\\n=== Distribución de los clústeres verdaderos ===\")\n",
        "print(df['True_Cluster'].value_counts(normalize=True))\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='True_Cluster', data=df)\n",
        "plt.title('Distribución de Clústeres Verdaderos')\n",
        "plt.xlabel('Clúster')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHiT4Bz7VeXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualización de las características (dispersión con clústeres verdaderos)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['Feature_1'], df['Feature_2'], c=df['True_Cluster'], cmap='viridis', s=50)\n",
        "plt.title('Distribución de Datos por Clústeres Verdaderos')\n",
        "plt.xlabel('Feature_1')\n",
        "plt.ylabel('Feature_2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iCrzoIzxVi0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Distribuciones de las características\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Feature_1'], kde=True)\n",
        "plt.title('Distribución de Feature_1')\n",
        "plt.xlabel('Feature_1')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['Feature_2'], kde=True)\n",
        "plt.title('Distribución de Feature_2')\n",
        "plt.xlabel('Feature_2')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PgAEzyBJVlZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Boxplots por clúster verdadero\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x='True_Cluster', y='Feature_1', data=df)\n",
        "plt.title('Feature_1 por Clúster Verdadero')\n",
        "plt.xlabel('Clúster')\n",
        "plt.ylabel('Feature_1')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='True_Cluster', y='Feature_2', data=df)\n",
        "plt.title('Feature_2 por Clúster Verdadero')\n",
        "plt.xlabel('Clúster')\n",
        "plt.ylabel('Feature_2')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5gGcO4tkVp1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Correlación entre características\n",
        "print(\"\\n=== Correlación entre características ===\")\n",
        "correlation = df[['Feature_1', 'Feature_2']].corr()\n",
        "print(correlation)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Matriz de Correlación')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lo38zjDGVsnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Análisis previo al clustering: Método del codo para determinar el número óptimo de clústeres\n",
        "inertias = []\n",
        "k_range = range(1, 10)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.title('Método del Codo para Determinar K')\n",
        "plt.xlabel('Número de Clústeres (K)')\n",
        "plt.ylabel('Inercia')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2RsP02SvVviI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Aplicar K-Means (como en el código original)\n",
        "kmeans = KMeans(n_clusters=4, random_state=0, n_init='auto')\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "y_kmeans"
      ],
      "metadata": {
        "id": "lj5wKOVzV3zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar los clústeres predichos\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
        "plt.title('Clustering K-Means (K=4)')\n",
        "plt.xlabel('Feature_1')\n",
        "plt.ylabel('Feature_2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iChFcikVV7UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el clustering\n",
        "inertia = kmeans.inertia_\n",
        "silhouette_avg = silhouette_score(X, y_kmeans)\n",
        "print(f\"\\n=== Evaluación del Clustering ===\")\n",
        "print(f\"Inercia: {inertia}\")\n",
        "print(f\"Puntuación de la silueta: {silhouette_avg}\")"
      ],
      "metadata": {
        "id": "0qFj0zmiV-5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}